---
title: "Final Project"
author: "Liz Austin, Casey Gibson, and Zhengfan Wang"
date: "November 15, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Lyme Disease CDC Data
Dataset detailing cases of Lyme Disease per 100,000 population in each state for the years 2006-2016

Data source:
https://www.cdc.gov/lyme_project/stats/index.html


Clustering by region reference:
https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf

###Casey
State space models
###Zhengfan
Clustering + Simulation

```{r, echo=FALSE}
library(ggplot2)
library(forecast)
library(ggfortify)
library(timeSeries)
library(stats)
lyme=read.csv("/Users/gcgibson/Desktop/lyme/lymedata.csv",header = T)
lyme$X2011<- as.integer(lyme$X2011)
lyme$X2012<- as.integer(lyme$X2012)

ts<- ts(t(lyme[,2:11]))


#Let 1 = Northeast
#Let 2 = Midwest
#Let 3 = South
#Let 4 = West
lyme$Region<- c(3,4,4,3,4,4,1,3,3,3,4,4,2,2,2,2,3,3,1,3,1,2,2,3,2,4,2,4,1,1,4,1,3,2,2,3,4,1,1,3,2,3,3,4,1,3,4,3,2,4)
lyme$Region<- as.factor(lyme$Region)

aggregate<- aggregate(cbind(X2006,X2007,X2008,X2009,X2010,X2011,X2012,X2013,X2014,X2015)~Region, data=lyme, sum)

ts2<- ts(t(aggregate[,2:11]))

par(mfrow=c(3,2))
par(mar=c(.5,4.5,.5,.5))
ccf(ts2[,1],ts2[,2],ylab="R1xR2")
ccf(ts2[,1],ts2[,3],ylab="R1xR3")
ccf(ts2[,1],ts2[,4],ylab="R1xR4")
ccf(ts2[,2],ts2[,3],ylab="R2xR3")
ccf(ts2[,2],ts2[,4],ylab="R2xR4")
ccf(ts2[,3],ts2[,4],ylab="R3xR4")
par(mfrow=c(1,1))
```

#Dataset sample 

```{r}
lyme[1:10,]
lyme[51:60,]
```
## Casey's part

####TODO
#####Model Fits
* Locally level model (DONE)
* AR model (DONE)
* Seasonal model (DONE)
* Temperature covariate model (DONE)
* Hierarchical model (DONE)
* Multiple covariate model
* Simple SIR model 
#####Model CV
* Compare residuals/ACF
* Investigate techniques for statistical inference on time varying regression coefficients


## Introduction
In order to examine the time-varying effects of Lyme disease incidence across the country we will employ the state space model framework. This will allow us to model both the underlying incidence of Lyme over time while accounting for correlation between observations. We first present a breif overview of state-space models and the types of inferences that can be made.

## Background
State space models are defined in terms of a given time-series of observations $Y_1,Y_2,...,Y_n$ and a set of latent factors $Z_1,Z_2,....Z_n$.

![](/Users/gcgibson/Downloads/figure1.gif)




This setup allows us to decompose a joint likleihood as follows.


$$L(z_1,z_2,...z_n | y_1,y_2,...,y_n) = p(z_1)\prod_{i=2}^n p(z_i |z_{i-1})p(y_i |z_i)$$


We can see that this decomposes into a series of product terms containing an observation model, a density describing the probability of an observation given a state, and a transition model, a density describing the probability of moving from one state to the next. In addition, all states obey the Markov property, meaning they are conditionally independent of all other states given the previous one. 

This set-up lends itself nicely to particle filtering, which we will treat as a black-box inference algorithm to get at the likelihood above. 

One further extension is to consider top-level unknown parameters. We can write the likelihood given a parameter vector $\theta$ as 
$$L(z_1,z_2,...z_n | y_1,y_2,...,y_n, \theta) = p(z_1 | \theta)\prod_{i=2}^n p(z_i |z_{i-1}, \theta)p(y_i |z_i, \theta)p(\theta)$$

To begin with, lets restrict our analysis to Massachusetts' data.


### Locally Level
A simple locally level model takes the form,
$$Z_t \sim N(Z_{t-1},\sigma^2)$$

$$Y_t \sim Pois(exp(X_t))$$
where we take our observation model to be $Poisson$ to restrict it to the integers. 

This model is called "locally level" because we can think of it as an intercept only regression model where each $Y_i$ is assigned an intercept $Z_i$ and these intercepts follow a Gaussian random walk.


```{r, echo=FALSE,message=FALSE}

ma_data <- array(unlist(lyme[which(lyme$State=="Massachusetts"),]))[2:11]

require(rbiips)
library(MCMCpack)
locally_level_model_file = '/Users/gcgibson/Desktop/lyme/pois.bug' # BUGS model filename

t_max = length(ma_data)
n_burn = 500 # nb of burn-in/adaptation iterations
n_iter = 1000 # nb of iterations after burn-in
thin = 5 # thinning of MCMC outputs
n_part = 50 # nb of particles for the SMC
param_names = c('log_sigma') # names of the variables updated with MCMC (others are updated with SMC)
latent_names = c('x') # names of the variables updated with SMC and that need to be monitored

inits = list(-2)

#setting the mean value of the initial count to 1400
locally_level_data = list(t_max=t_max, y = ma_data,  mean_x_init=log(1400))
locally_level_model = biips_model(locally_level_model_file, data=locally_level_data,sample_data = FALSE)

locally_level_obj_pmmh = biips_pmmh_init(locally_level_model, param_names, inits=inits,
                           latent_names=latent_names) # creates a pmmh object

biips_pmmh_update(locally_level_obj_pmmh, n_burn, n_part) # adaptation and burn-in iterations


locally_level_out_pmmh = biips_pmmh_samples(locally_level_obj_pmmh, n_iter, n_part, thin=thin) # samples
locally_level_model_summary = biips_summary(locally_level_out_pmmh, probs=c(.025, .975))


p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(locally_level_model_summary$x$mean)),y=exp(locally_level_model_summary$x$mean)), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
 geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=ma_data),aes(x=x,ymin=exp(locally_level_model_summary$x$quant$`0.025`),ymax=exp(locally_level_model_summary$x$quant$`0.975`)),alpha=0.3)+
  xlab('data_date') +
  ylab('count')
print(p)

print ("Estimted log variance")
print (locally_level_model_summary$log_sigma)



```

According to [1] the best diagonstic tool for evaluating state-space model fit is a residual analysis on the one-step-ahead predictions. In other words, we evaluate the sequence $$\hat{Y_t} -Y_t | \theta_{t-1} $$


where $$\hat{Y_t} = E(Y_t | \theta_{t-1}) = E( E(Y_t| \theta_{t-1},\theta_t))$$ by the tower property of expectation.
However, looking at the graphical model above, we see that $Y_t$ is conditionally independent of $\theta_{t-1}$ given $\theta_t$. Therefore, we can re-write this as 

$$\hat{Y_t} = E(E(Y_t | \theta_t))$$

Which in the context of a poisson locally linear model is simply, 

$$e^{\theta_t}=e^{\theta_{t-1}}$$
since $E(\theta_t) = \theta_{t-1}$. 

We can now evaluate the one-step ahead predictions

```{r, message=FALSE,echo=FALSE}

locally_level_residuals <- c()

for (i in seq(2:length(ma_data))){
  locally_level_residuals <- c(locally_level_residuals, (ma_data[i] - exp(locally_level_model_summary$x$mean[i-1] )))
}

qqnorm(scale(locally_level_residuals))
qqline(scale(locally_level_residuals))

```


We can see that locally-level model is able to almost perfectly capture the data in the plot above, but the residuals are far from $N(0,1)$. Although this model is a good baseline, it offers little in terms of inference or predicatbility. In the absence of any additional information, the model predicts that the true underlying incidence is simply last years incidence. 

### Seasonal


We next impose our knowledge of the seasonal effects of lyme disease onto the state-space model by incorporating the following trasformation 

$$\begin{bmatrix}Z_{t,1} \\ Z_{t,2} \end{bmatrix}  \sim N(\begin{bmatrix} cos(2\pi/4)  & sin(2\pi/4) \\  -sin(2\pi/4) &cos(2\pi/4) \end{bmatrix}  \begin{bmatrix}Z_{t-1,1} \\ Z_{t-1,2} \end{bmatrix}  ,\begin{bmatrix} \sigma^2 \\0 \end{bmatrix})$$


This imposes a seasonal cyclic dependence of $1$ year on the true underlying state.



```{r,echo=FALSE,message=FALSE}


require(rbiips)
library(MCMCpack)
seasonal_model_file = '/Users/gcgibson/Desktop/lyme/seasonal_pois.bug' # BUGS model filename

seasonal_t_max = length(ma_data)
n_burn = 5000 # nb of burn-in/adaptation iterations
n_iter = 10000 # nb of iterations after burn-in
thin = 5 # thinning of MCMC outputs
n_part = 50 # nb of particles for the SMC
latent_names = c('x') # names of the variables updated with SMC and that need to be monitored

inits = list(-2)

seasonality =6
G = matrix(c(cos(2*pi/seasonality),sin(2*pi/seasonality),-sin(2*pi/seasonality),cos(2*pi/seasonality)), nrow=2, byrow=TRUE)

#setting the mean value of the initial count to 1400
seasonal_data = list(t_max=t_max, y = ma_data,  G = G, mean_sigma_init = c(0,0), cov_sigma_init=.001*diag(2) ,mean_x_init=c(log(1400),log(1400)))
seasonal_model = biips_model(seasonal_model_file, data=seasonal_data,sample_data = FALSE)

##fixing variance for now, will extend model to handle inference over variance later

n_part = 100000 # Number of particles
variables = c('x') # Variables to be monitored
seasonal_out_smc = biips_smc_samples(seasonal_model, variables, n_part)
seasonal_model_summary = biips_summary(seasonal_out_smc, probs=c(.025, .975))



p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(seasonal_model_summary$x$f$mean[1,])),y=exp(seasonal_model_summary$x$f$mean[1,])), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
 geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=exp(seasonal_model_summary$x$f$mean[1,])),aes(x=x,ymin=exp(seasonal_model_summary$x$f$quant$`0.025`[1,]),ymax=exp(seasonal_model_summary$x$f$quant$`0.975`[1,])),alpha=0.3)+
  xlab('data_date') +
  ylab('count') + ylim(low=0,high=20000)
print(p)


```

 We can again evaluate one step ahead forecasts using the following set of equations. 

$$E(\vec{X_{t}} |\vec{X_{t-1}})  =  G\vec{x_{t-1}}$$
$$E(Y_t | X_{t,1}) = exp(cos(2\pi/6)x_{t,1} + sin(2\pi/6)x_{t,2})$$


```{r, message=FALSE, echo=FALSE}
seasonal_residuals <- c()

for (i in seq(2:length(ma_data))){
  seasonal_residuals <- c(seasonal_residuals, (ma_data[i] - exp(cos(2*pi/6)*seasonal_model_summary$x$f$mean[1,i-1] + 
                                                                  sin(2*pi/6)*seasonal_model_summary$x$f$mean[2,i-1])))
}

qqnorm(scale(seasonal_residuals))
qqline(scale(seasonal_residuals))
```


The seasonal model also seems to perform well on the data, however the residuals do not seem to be $N(0,1)$. 




### Auto-regressive model
We can further extend the interpritability of our latent states by incorporating an auto-regressive term. 
The AR-SSM take sthe following form 


$$X_t \sim N(\phi_1X_{t-1} +\phi_2 X_{t-2},\sigma^2)$$


$$Y_t \sim Pois(exp(X_t))$$
We can perform top level inference on the parameters $\phi_1,\phi_2$ using PMMH (Particle Marginal Metropolis Hastings), which allows us to subsitutite a particle filter likelihood into the regular MH algorithm given a draw from a proposal distribution over $\phi_1$ and $\phi_2$

```{r,echo=FALSE,message=FALSE}


require(rbiips)
library(MCMCpack)
ar_model_file = '/Users/gcgibson/Desktop/lyme/ar_dlm.bug' # BUGS model filename


t_max = length(ma_data)
n_burn = 500 # nb of burn-in/adaptation iterations
n_iter = 1000 # nb of iterations after burn-in
thin = 5 # thinning of MCMC outputs
n_part = 50 # nb of particles for the SMC
param_names = c('phi1','phi2') # names of the variables updated with MCMC (others are updated with SMC)
latent_names = c('x') # names of the variables updated with SMC and that need to be monitored

ar_data = list(t_max=t_max, y = ma_data,mean_x_init=c(log(1400),log(1400)))

ar_model = biips_model(ar_model_file, ar_data, sample_data=FALSE) # Create Biips model and sample data


ar_obj_pmmh = biips_pmmh_init(ar_model, param_names, inits=list(phi1=.1,phi2=.1),
                     latent_names=latent_names) # creates a pmmh object

biips_pmmh_update(ar_obj_pmmh, n_burn, n_part) # adaptation and burn-in iterations

ar_out_pmmh = biips_pmmh_samples(ar_obj_pmmh, n_iter, n_part, thin=thin) # samples

ar_summ_pmmh = biips_summary(ar_out_pmmh, probs=c(.025, .975))


p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(ma_data)),y=exp(ar_summ_pmmh$x$mean)), aes(x = x, y = y), color = "red") +
  
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "cornflowerblue") +
 geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=ma_data),aes(x=x,ymin=exp(ar_summ_pmmh$x$quant$`0.025`),ymax=exp(ar_summ_pmmh$x$quant$`0.975`)),alpha=0.3)+
  xlab('data_date') +
  ylab('count') 
print(p)

```

The AR-model seems to both fit the best, and contain the most reasonable confidence intervals. The estimated value of 
$$\phi_1 = .967$$
$$\phi_2 = -0.019$$


Again, we can evalute the one step ahead forecasts of the AR model. 

$$E(Y_t | Z_t) = exp(Z_t)$$
$$E(\vec{Z_t} | \vec{Z_{t-1}}) = \phi_1*Z_{1,t-1} + \phi_2*Z_{2,t-1}$$  
```{r, message=FALSE, echo=FALSE}
ar_residuals <- c()

for (i in seq(2:length(ma_data))){
  ar_residuals <- c(ar_residuals, (ma_data[i] - exp(ar_summ_pmmh$phi1$mean*ar_summ_pmmh$x$mean[i-1] + 
                                                                  ar_summ_pmmh$phi2$mean*ar_summ_pmmh$x$mean[i-2])))
}

qqnorm(scale(ar_residuals))
qqline(scale(ar_residuals))
```
The AR model also has the best standardized residual.



### Dynamic Regression
In order to incorporate additional sources of information, we next consider the case where we have an observed series of temperature over the duration of lyme disease incidence. 

This model takes the form 

$$\beta_t \sim N(\beta_{t-1},\sigma^2)$$


$$Y_t \sim Pois(Temp_t*exp(\beta_t))$$

In other words, we can now interpet the $Z's$ as time varying regression coefficients that have a multiplicitaive effect of $exp(\beta_t)$ on the temperature.

In order to evaluate this model, we first generate random temperature draws. 

```{r,echo=FALSE,message=FALSE}


# simulated temperature in celcius 
temp <- c(12,18,14,17,24,21,26,22,17,23)
# estimated deer population in thousands
deer <- c(90,87,92,79,81,85,78,80,75,82)
# estimated rainfall
rainfall <- c(36,38,32,34,35,35,39,31,33,36)


require(rbiips)
library(MCMCpack)


reg_model_file = '/Users/gcgibson/Desktop/lyme/pois_with_reg.bug' # BUGS model filenam
t_max = length(ma_data)
n_burn = 5000 # nb of burn-in/adaptation iterations
n_iter = 10000 # nb of iterations after bNNNurn-in
thin = 5 # thinning of MCMC outputs
n_part = 50 # nb of particles for the SMC
reg_param_names = c('log_sigma_temp','log_sigma_deer','log_sigma_rainfall') # names of the variables updated with MCMC (others are updated with SMC)Ns
reg_latent_names = c('x') # names of the variables updated with SMC and that need to be monitored

reg_inits = list(1,1,1)
reg_data = list(t_max=t_max, y = ma_data, temp = temp, rainfall = rainfall, deer=deer, mean_x_init=log(1400))
reg_model = biips_model(reg_model_file, data=reg_data,sample_data = FALSE)

reg_obj_pmmh = biips_pmmh_init(reg_model, reg_param_names, inits=reg_inits,
                           latent_names=reg_latent_names) # creates a pmmh object


biips_pmmh_update(reg_obj_pmmh, n_burn, n_part) # adaptation and burn-in iterations
reg_out_pmmh = biips_pmmh_samples(reg_obj_pmmh, n_iter, n_part, thin=thin) # samples

reg_summ_pmmh = biips_summary(reg_out_pmmh, probs=c(.025, .975))

```

We can now examine the fit of the regression model.
```{r,echo=FALSE,message=FALSE}

p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(ma_data)),y=exp(log(temp)*reg_summ_pmmh$x$mean[1,] + log(deer)*reg_summ_pmmh$x$mean[2,] + log(rainfall)*reg_summ_pmmh$x$mean[3,])), aes(x = x, y = y), color = "red") +
  geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "blue") +
geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=ma_data),aes(x=x,ymin=exp(log(temp)*reg_summ_pmmh$x$quant$`0.025`[1,] + log(deer)*reg_summ_pmmh$x$quant$`0.025`[2,] + log(rainfall)*reg_summ_pmmh$x$quant$`0.025`[3,]),ymax=exp(log(temp)*reg_summ_pmmh$x$quant$`0.975`[1,] + log(deer)*reg_summ_pmmh$x$quant$`0.975`[2,] + log(rainfall)*reg_summ_pmmh$x$quant$`0.975`[3,])),alpha=0.3)+
  xlab('data_date') +
  ylab('count')
print(p)


```

We can see that in the presence of the regressors the fit remains good. If we investigate the latent state of lyme, we can now interpret the $\theta_t$'s as dynamic regression coefficients. If we first scale the coefficients to have unit norm we can get a clear picture of the trend. 

```{r,echo=FALSE,message=FALSE}
p<- ggplot() 
p<- p+ geom_line(data = data.frame(x=seq(1,length(summ_pmmh$x$mean)),y=scale(exp(summ_pmmh$x$mean[1,]))), aes(x = x, y = y), color = "red") +
 # geom_line(data=data.frame(x=seq(1,length(ma_data)),y=ma_data), aes(x = x, y = y), color = "blue") +
#geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=ma_data),aes(x=x,ymin=summ_pmmh$x$quant$`0.025`,ymax=summ_pmmh$x$quant$`0.975`),alpha=0.3)+
  xlab('data_date') +
  ylab('count')
print(p)
```

We can also perform inference using the posterior distribution over $\beta_{1,t} \ \forall t$ where

$$H_0: \text{Temperature has no effect on Lyme incidence at any point in time }$$
$$ \beta_{1,t} = 0 \ \forall t$$
$$H_a: \beta_{1,t} \neq 0 \text{ for some } t$$

```{r}
print (summ_pmmh$x$quant$`0.025`)
print (summ_pmmh$x$quant$`0.975`)
```

We can see that the last three of the credible intervals contain $0$ so we cannot reject the null hypothesis at the $\alpha = .05$ level. 


We can repeat the previous analysis for $\beta_{2,t}$ and $\beta_{3,t}$, finding again that none of the regressors can consistently explain the variation in Lyme disease incidence over all time points. However, this is a relatively strong hypothesis, and is also based on very limited data, so no conclusions should be drawn until further data is collected.


### Hierarchical
In order to leverage the hierarchical nature of the data we consider an extension to hierarchical dynamic models taking the form 

$$Z_t \sim N(Z_{t-1},\sigma^2)$$
$$X_{tj} \sim N(Z_t,1)$$
$$Y_{tj} \sim Pois(exp(X_{tj}))$$

where $j$ indexes region $j = [1,2,3,4]$.

The intution here is that there is some true country wide lyme incidence, and regional variational around this country level.


```{r,echo=FALSE,message=FALSE}



require(rbiips)
library(MCMCpack)


model_file = '/Users/gcgibson/Desktop/lyme/hierarchical.bug' # BUGS model filename

par(bty='l')
light_blue = rgb(.7, .7, 1)
light_red = rgb(1, .7, .7)


t_max = nrow(ts2)
n_burn = 500 # nb of burn-in/adaptation iterations
n_iter = 1000 # nb of iterations after burn-in
thin = 5 # thinning of MCMC outputs
n_part = 10 # nb of particles for the SMC
param_names = c('log_sigma','log_sigma_r1','log_sigma_r2','log_sigma_r3','log_sigma_r4') # names of the variables updated with MCMC (others are updated with SMC)
latent_names = c('x','z') # names of the variables updated with SMC and that need to be monitored

inits = list(2,2,2,2,2)
data = list(t_max=t_max, y = t(ts2),   regional_mean=0)
model = biips_model(model_file, data=data,sample_data = FALSE)

obj_pmmh = biips_pmmh_init(model, param_names, inits=inits,
                           latent_names=latent_names) # creates a pmmh object


biips_pmmh_update(obj_pmmh, n_burn, n_part) # adaptation and burn-in iterations
out_pmmh = biips_pmmh_samples(obj_pmmh, n_iter, n_part, thin=thin) # samples
summ_pmmh = biips_summary(out_pmmh, probs=c(.025, .975))

print("Global log precision ")
print (summ_pmmh$log_sigma$mean)
print (summ_pmmh$log_sigma_r1$mean)
print (summ_pmmh$log_sigma_r2$mean)
print (summ_pmmh$log_sigma_r3$mean)
print (summ_pmmh$log_sigma_r4$mean)
```
We see that the the region with the highest variance (lowest precision) is region $1$, which is the midtwest region, and the lowest variance (highest precision) is region $4$. This is consistent with the observed data. 

The estimated true country level level of lyme is

```{r,echo=FALSE,message=FALSE}
#p<- ggplot() 
#p<- p+ geom_line(data = data.frame(x=seq(1,length(summ_pmmh$z$mean)),y=exp(summ_pmmh$z$mean)), aes(x = x, y = y), color = "red") +
 # geom_line(data=data.frame(x=seq(1,length(ma_data)),y=row_sum), aes(x = x, y = y), color = "blue") +
#geom_ribbon(data=data.frame(x=seq(1,length(ma_data)),y=ma_data),aes(x=x,ymin=summ_pmmh$x$quant$`0.025`,ymax=summ_pmmh$x$quant$`0.975`),alpha=0.3)+
#  xlab('data_date') +
#  ylab('count')
#print(p)
```


References
[1] G. Petris et al., Dynamic Linear Models with R, Use R, DOI: 10.1007b135794_2,
 


